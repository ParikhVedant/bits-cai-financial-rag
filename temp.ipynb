{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b8842dd",
   "metadata": {},
   "source": [
    "# CAI Assignment 2 - Updated\n",
    "\n",
    "This notebook now displays candidate answers with confidence scores and implements a re-ranking mechanism. The re-ranking sorts the answers in descending order of confidence so that the top answer (with the highest score) is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f021c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_confidence(answer, question, context):\n",
    "    \"\"\"Compute a confidence score for an answer candidate.\n",
    "    In a production system, this function could use the model's logit outputs,\n",
    "    a softmax probability, or other similarity metrics between the answer and\n",
    "    the question/context. Here we simulate it using a random value.\"\"\"\n",
    "    return np.random.rand()\n",
    "\n",
    "# Example candidate answers (in a real system these would come from your QA model)\n",
    "candidate_answers = [\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"France has Paris as its capital.\"\n",
    "]\n",
    "\n",
    "# Dummy question and context\n",
    "question = \"What is the capital of France?\"\n",
    "context = \"France is a country in Europe. Its capital is Paris.\"\n",
    "\n",
    "# Compute confidence scores for each candidate answer\n",
    "answers_with_scores = []\n",
    "for answer in candidate_answers:\n",
    "    score = compute_confidence(answer, question, context)\n",
    "    answers_with_scores.append((answer, score))\n",
    "\n",
    "# Re-rank candidate answers based on the confidence score (higher is better)\n",
    "answers_with_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display all candidate answers with their confidence scores\n",
    "print(\"Candidate Answers with Confidence Scores:\")\n",
    "for ans, score in answers_with_scores:\n",
    "    print(f\"Answer: {ans}\\t Confidence Score: {score:.2f}\")\n",
    "\n",
    "# Optionally, select and display the top answer\n",
    "top_answer, top_score = answers_with_scores[0]\n",
    "print(\"\\nTop Answer Selected:\")\n",
    "print(f\"Answer: {top_answer}\\t Confidence Score: {top_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05f33c4",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "1. **Computing Confidence Scores:**  \n",
    "   The function `compute_confidence` currently returns a random number for demonstration purposes. In a real implementation, you might derive this score from your model's output probabilities (for example, the softmax value for the answer tokens) or by computing a similarity between the question and the answer.\n",
    "\n",
    "2. **Re-ranking:**  \n",
    "   After computing the scores for all candidate answers, the list is re-sorted (re-ranked) in descending order of confidence. The answer with the highest score is then selected as the top answer.\n",
    "\n",
    "3. **Display:**  \n",
    "   The notebook prints each candidate answer along with its confidence score. It then prints the top answer separately.\n",
    "\n",
    "Feel free to replace the dummy confidence computation with your actual implementation. Also, if your model already provides candidate answers with scores, you can adapt the re-ranking step accordingly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
